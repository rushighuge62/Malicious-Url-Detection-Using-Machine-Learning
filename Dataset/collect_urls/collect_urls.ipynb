{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Collected 7410 safe URLs and saved to CSV!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_safe_urls(seed_urls, max_urls=7000):\n",
    "    visited = set()\n",
    "    to_visit = set(seed_urls)\n",
    "    all_links = set()\n",
    "\n",
    "    while to_visit and len(all_links) < max_urls:\n",
    "        url = to_visit.pop()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link[\"href\"]\n",
    "                if href.startswith(\"/wiki/\") and \":\" not in href:\n",
    "                    to_visit.add(\"https://en.wikipedia.org\" + href)\n",
    "                elif \"http\" in href:\n",
    "                    all_links.add(href)\n",
    "            \n",
    "            visited.add(url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "    return list(all_links)\n",
    "\n",
    "# Seed pages\n",
    "seed_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_most_popular_websites\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_search_engines\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_social_networking_websites\"\n",
    "]\n",
    "\n",
    "# Get Wikipedia safe URLs\n",
    "safe_urls = get_wikipedia_safe_urls(seed_urls, max_urls=7000)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(safe_urls, columns=[\"url\"])\n",
    "df[\"type\"] = \"benign\"\n",
    "df.to_csv(\"wikipedia_safe_urls.csv\", index=False)\n",
    "\n",
    "print(f\"✔ Collected {len(safe_urls)} safe URLs and saved to CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Collected 8934 phishing URLs and saved to CSV!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# List of phishing URL sources\n",
    "PHISHING_FEEDS = [\n",
    "    \"https://openphish.com/feed.txt\",\n",
    "    # \"http://data.phishtank.com/data/online-valid.csv\",\n",
    "    \"https://urlhaus.abuse.ch/downloads/text_online/\",\n",
    "]\n",
    "\n",
    "def get_phishing_urls():\n",
    "    phishing_urls = set()  # Use a set to avoid duplicates\n",
    "\n",
    "    for feed in PHISHING_FEEDS:\n",
    "        try:\n",
    "            response = requests.get(feed, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text.split(\"\\n\")\n",
    "                for line in content:\n",
    "                    line = line.strip()\n",
    "                    if line and (\"http\" in line or \".\" in line):  # Ensure it's a valid URL\n",
    "                        phishing_urls.add(line)\n",
    "            else:\n",
    "                print(f\"❌ Failed to fetch from {feed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error fetching {feed}: {e}\")\n",
    "\n",
    "    return list(phishing_urls)\n",
    "\n",
    "# Get phishing URLs\n",
    "phishing_urls = get_phishing_urls()\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(phishing_urls, columns=[\"url\"])\n",
    "df[\"type\"] = \"phishing\"\n",
    "df.to_csv(\"phishing_urls.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✔ Collected {len(phishing_urls)} phishing URLs and saved to CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Collected 8439 defacement URLs and saved to CSV!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# List of defacement URL sources\n",
    "DEFACEMENT_FEEDS = [\n",
    "    \"https://cybercrime-tracker.net/ccamlist.php\",\n",
    "    \"https://urlhaus.abuse.ch/downloads/text_online/\",\n",
    "    # \"http://www.malwaredomainlist.com/mdlcsv.php\",\n",
    "    \"https://feodotracker.abuse.ch/downloads/ipblocklist_recommended.txt\"\n",
    "]\n",
    "\n",
    "def get_defacement_urls():\n",
    "    defacement_urls = set()  # Use a set to remove duplicates\n",
    "\n",
    "    for feed in DEFACEMENT_FEEDS:\n",
    "        try:\n",
    "            response = requests.get(feed, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text.split(\"\\n\")\n",
    "                for line in content:\n",
    "                    line = line.strip()\n",
    "                    if line and (\"http\" in line or \".\" in line):  # Ensure it's a valid URL\n",
    "                        defacement_urls.add(line)\n",
    "            else:\n",
    "                print(f\"❌ Failed to fetch from {feed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error fetching {feed}: {e}\")\n",
    "\n",
    "    return list(defacement_urls)\n",
    "\n",
    "# Get defacement URLs\n",
    "defacement_urls = get_defacement_urls()\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(defacement_urls, columns=[\"url\"])\n",
    "df[\"type\"] = \"defacement\"\n",
    "df.to_csv(\"defacement_urls.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✔ Collected {len(defacement_urls)} defacement URLs and saved to CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Final dataset with real benign URLs saved!\n"
     ]
    }
   ],
   "source": [
    "# Load malicious dataset\n",
    "df_malicious = pd.read_csv(\"malicious_phish.csv\")\n",
    "\n",
    "# Load verified safe URLs\n",
    "df_safe = pd.read_csv(\"wikipedia_safe_urls.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "df_final = pd.concat([df_malicious, df_safe], ignore_index=True)\n",
    "\n",
    "# Save final dataset\n",
    "df_final.to_csv(\"final_url_dataset.csv\", index=False)\n",
    "\n",
    "print(\"✔ Final dataset with real benign URLs saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Collected 147089 malware URLs and saved to CSV!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# List of malware URL sources\n",
    "MALWARE_FEEDS = [\n",
    "    \"https://urlhaus.abuse.ch/downloads/text/\",\n",
    "    # \"https://bazaar.abuse.ch/export/txt/recent/\",\n",
    "    # \"http://www.malwaredomainlist.com/mdlcsv.php\",\n",
    "    \"https://feodotracker.abuse.ch/downloads/ipblocklist_recommended.txt\",\n",
    "    # \"https://cybercrime-tracker.net/all.php\",\n",
    "    \"http://vxvault.net/URL_List.php\"\n",
    "]\n",
    "\n",
    "def get_malware_urls():\n",
    "    malware_urls = set()  # Use a set to avoid duplicates\n",
    "\n",
    "    for feed in MALWARE_FEEDS:\n",
    "        try:\n",
    "            response = requests.get(feed, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                content = response.text.split(\"\\n\")\n",
    "                for line in content:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.startswith(\"#\") and (\"http\" in line or \".\" in line):  # Clean URLs\n",
    "                        malware_urls.add(line)\n",
    "            else:\n",
    "                print(f\"❌ Failed to fetch from {feed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error fetching {feed}: {e}\")\n",
    "\n",
    "    return list(malware_urls)\n",
    "\n",
    "# Get malware URLs\n",
    "malware_urls = get_malware_urls()\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(malware_urls, columns=[\"url\"])\n",
    "df[\"type\"] = \"malware\"\n",
    "df.to_csv(\"malware_urls.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✔ Collected {len(malware_urls)} malware URLs and saved to CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://en.wikipedia.org/wiki/List_of_most_popular_websites (0/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Main_Page (54/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:Contents (87/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Portal:Current_events (90/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:Random (207/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:About (218/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Help:Contents (239/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Help:Introduction (255/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:Community_portal (261/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:RecentChanges (339/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard (342/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:SpecialPages (350/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:Search (351/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:MyContributions (351/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:MyTalk (364/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/List_of_most-visited_websites (364/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Talk:List_of_most-visited_websites (364/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:WhatLinksHere/List_of_most-visited_websites (372/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:RecentChangesLinked/List_of_most-visited_websites (374/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:Protection_policy#semi (375/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/List_of_social_platforms_with_at_least_100_million_active_users (396/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Lists#Dynamic_lists (438/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Special:EditPage/List_of_most-visited_websites (508/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources (508/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Similarweb (567/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Semrush (601/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Nonprofit_organization (635/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Google_Search (716/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Google (1083/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/YouTube (1865/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Facebook (2452/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Meta_Platforms (3144/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Instagram (3579/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/WhatsApp (4491/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Twitter (5206/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/X_Corp. (6084/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikipedia (6210/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Wikimedia_Foundation (7130/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/ChatGPT (7551/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/OpenAI (8184/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Reddit (8786/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Yahoo (9573/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Yahoo!_Inc._(2017%E2%80%93present) (9840/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Yahoo!_Japan (9934/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/LY_Corporation (9973/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Amazon_(company) (9994/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Yandex (10449/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Baidu (10726/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/TikTok (10976/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/ByteDance (11831/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Netflix (12153/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/MSN (12974/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Microsoft (13105/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Microsoft_Bing (13696/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Pornhub (14027/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/Aylo (14317/15000)\n",
      "Fetching: https://en.wikipedia.org/wiki/LinkedIn (14566/15000)\n",
      "✔ 15000 Safe URLs saved!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_wikipedia_links(start_url, limit=15000):\n",
    "    \"\"\"\n",
    "    Crawls Wikipedia pages to collect external links (safe URLs).\n",
    "    \"\"\"\n",
    "    queue = [start_url]  # Start from this URL\n",
    "    visited = set()\n",
    "    safe_urls = set()\n",
    "\n",
    "    while queue and len(safe_urls) < limit:\n",
    "        url = queue.pop(0)  # Get next URL to process\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        print(f\"Fetching: {url} ({len(safe_urls)}/{limit})\")\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Extract external links\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    href = link[\"href\"]\n",
    "\n",
    "                    # Convert relative Wikipedia links to absolute\n",
    "                    if href.startswith(\"/wiki/\"):\n",
    "                        full_url = \"https://en.wikipedia.org\" + href\n",
    "                        if full_url not in visited and len(queue) < 500:  # Avoid infinite loops\n",
    "                            queue.append(full_url)\n",
    "\n",
    "                    # Extract external safe links\n",
    "                    if href.startswith(\"http\") and \"wikipedia.org\" not in href:\n",
    "                        safe_urls.add(href)\n",
    "\n",
    "                    if len(safe_urls) >= limit:\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                print(f\"❌ Failed to fetch {url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error fetching {url}: {e}\")\n",
    "\n",
    "        time.sleep(1)  # Respect Wikipedia’s servers\n",
    "\n",
    "    return list(safe_urls)\n",
    "\n",
    "# List of Wikipedia pages to start crawling from\n",
    "seed_pages = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_most_popular_websites\",\n",
    "    \"https://en.wikipedia.org/wiki/Category:Websites\",\n",
    "    \"https://en.wikipedia.org/wiki/Category:Online_services\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_social_media_platforms\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_video_sharing_websites\",\n",
    "    \"https://en.wikipedia.org/wiki/List_of_news_websites\",\n",
    "]\n",
    "\n",
    "# Collect Safe URLs\n",
    "safe_urls = set()\n",
    "for page in seed_pages:\n",
    "    safe_urls.update(get_wikipedia_links(page, limit=15000))\n",
    "    if len(safe_urls) >= 15000:\n",
    "        break\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(list(safe_urls), columns=[\"url\"])\n",
    "df[\"type\"] = \"benign\"\n",
    "df.to_csv(\"safe_urls.csv\", index=False)\n",
    "\n",
    "print(f\"✔ {len(safe_urls)} Safe URLs saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching phishing URLs from https://openphish.com/feed.txt...\n",
      "Fetching malware URLs from https://urlhaus.abuse.ch/downloads/text/...\n",
      "Fetching defacement URLs from https://cybercrime-tracker.net/ccamlist.php...\n",
      "✔ 5944 Malicious URLs saved!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_malicious_urls(source_url, limit=5000):\n",
    "    \"\"\"\n",
    "    Fetches malicious URLs from a given threat intelligence feed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(source_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            urls = response.text.split(\"\\n\")\n",
    "            urls = [u.strip() for u in urls if u.strip() and not u.startswith(\"#\")]\n",
    "            return urls[:limit]  # Limit results to avoid excessive duplicates\n",
    "        else:\n",
    "            print(f\"❌ Failed to fetch: {source_url}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error fetching {source_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Real-time Malicious URL sources\n",
    "malicious_sources = {\n",
    "    \"phishing\": \"https://openphish.com/feed.txt\",\n",
    "    \"malware\": \"https://urlhaus.abuse.ch/downloads/text/\",\n",
    "    \"defacement\": \"https://cybercrime-tracker.net/ccamlist.php\",\n",
    "}\n",
    "\n",
    "# Fetch malicious URLs\n",
    "malicious_urls = []\n",
    "for category, url in malicious_sources.items():\n",
    "    print(f\"Fetching {category} URLs from {url}...\")\n",
    "    malicious_urls.extend([(u, category) for u in fetch_malicious_urls(url, limit=5000)])\n",
    "    time.sleep(2)  # Respect servers\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(malicious_urls, columns=[\"url\", \"type\"])\n",
    "df.to_csv(\"malicious_urls.csv\", index=False)\n",
    "\n",
    "print(f\"✔ {len(malicious_urls)} Malicious URLs saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Combined 177816 rows into 'final_url_dataset.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\"wikipedia_safe_urls.csv\", \"phishing_urls.csv\", \"defacement_urls.csv\", \"malware_urls.csv\",\"malicious_phish.csv\"]\n",
    "\n",
    "# Read and combine all CSV files\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save to a new CSV file\n",
    "combined_df.to_csv(\"final_url_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"✔ Combined {len(combined_df)} rows into 'final_url_dataset.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Encoded URLs saved to final_encoded_urls.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def encode_url(url):\n",
    "    \"\"\"Convert a URL into ASCII Hex Encoding (UTF-8 Safe).\"\"\"\n",
    "    return url.encode('utf-8').hex()  # UTF-8 encoding prevents errors\n",
    "\n",
    "# Load the final URLs CSV\n",
    "df = pd.read_csv(\"final_url_dataset.csv\")\n",
    "\n",
    "# Drop any rows with missing URLs\n",
    "df = df.dropna(subset=[\"url\"])\n",
    "\n",
    "# Encode URLs safely\n",
    "df[\"encoded_url\"] = df[\"url\"].apply(encode_url)\n",
    "\n",
    "# Save the encoded URLs to a new CSV\n",
    "df[[\"encoded_url\", \"type\"]].to_csv(\"final_encoded_urls.csv\", index=False)\n",
    "\n",
    "print(f\"✔ Encoded URLs saved to final_encoded_urls.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
